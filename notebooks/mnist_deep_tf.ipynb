{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST data analysis (Tensorflow)\n",
    "This notebook uses pure Tensorflow to do digit classification using the [MNIST data set](http://yann.lecun.com/exdb/mnist/). MNIST is a labeled set of images of handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check first the TensorFlow version and the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "import socket\n",
    "print(socket.gethostname())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "#SOURCE_URL = 'https://storage.googleapis.com/cvdf-datasets/mnist/'\n",
    "SOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'\n",
    "\n",
    "#DATA_DIR = \"/tmp/mnist-data\"\n",
    "DATA_DIR = \"../data/raw\"\n",
    "\n",
    "def maybe_download(filename):\n",
    "    \"\"\"A helper to download the data files if not present.\"\"\"\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        os.mkdir(DATA_DIR)\n",
    "    filepath = os.path.join(DATA_DIR, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        filepath, _ = urlretrieve(SOURCE_URL + filename, filepath)\n",
    "        statinfo = os.stat(filepath)\n",
    "        print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "    else:\n",
    "        print('Already downloaded', filename)\n",
    "    return filepath\n",
    "\n",
    "train_data_filename = maybe_download('train-images-idx3-ubyte.gz')\n",
    "train_labels_filename = maybe_download('train-labels-idx1-ubyte.gz')\n",
    "test_data_filename = maybe_download('t10k-images-idx3-ubyte.gz')\n",
    "test_labels_filename = maybe_download('t10k-labels-idx1-ubyte.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us read the first image from the test data as a sanity check and show it as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, binascii, struct, numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with gzip.open(test_data_filename) as f:\n",
    "    # Print the header fields.\n",
    "    for field in ['magic number', 'image count', 'rows', 'columns']:\n",
    "        # struct.unpack reads the binary data provided by f.read.\n",
    "        # The format string '>i' decodes a big-endian integer, which\n",
    "        # is the encoding of the data.\n",
    "        print(field, struct.unpack('>i', f.read(4))[0])\n",
    "    \n",
    "    # Read the first 28x28 set of pixel values. \n",
    "    # Each pixel is one byte, [0, 255], a uint8.\n",
    "    buf = f.read(28 * 28)\n",
    "    image = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# We'll show the image and its pixel value histogram side-by-side.\n",
    "_, ax1 = plt.subplots(1, 1)\n",
    "\n",
    "# To interpret the values as a 28x28 image, we need to reshape\n",
    "# the numpy array, which is one dimensional.\n",
    "ax1.imshow(image.reshape(28, 28), cmap=plt.cm.Greys);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us read the whole data set and store it in 'mnist' variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# N.B. using maybe_download() from above is not necessary, input_data.read_data_sets() can do it all\n",
    "mnist = input_data.read_data_sets(DATA_DIR, one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Convolutional network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First define functions for weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "  \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layer ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool_2x2(x):\n",
    "  \"\"\"max_pool_2x2 downsamples a feature map by 2X.\"\"\"\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now build the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepnn(x):\n",
    "  \"\"\"deepnn builds the graph for a deep net for classifying digits.\n",
    "  Args:\n",
    "    x: an input tensor with the dimensions (N_examples, 784), where 784 is the\n",
    "    number of pixels in a standard MNIST image.\n",
    "  Returns:\n",
    "    A tuple (y, keep_prob). y is a tensor of shape (N_examples, 10), with values\n",
    "    equal to the logits of classifying the digit into one of 10 classes (the\n",
    "    digits 0-9). keep_prob is a scalar placeholder for the probability of\n",
    "    dropout.\n",
    "  \"\"\"\n",
    "  # Reshape to use within a convolutional neural net.\n",
    "  # Last dimension is for \"features\" - there is only one here, since images are\n",
    "  # grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n",
    "  x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "  # First convolutional layer - maps one grayscale image to 32 feature maps.\n",
    "  W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "  b_conv1 = bias_variable([32])\n",
    "  h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "  # Pooling layer - downsamples by 2X.\n",
    "  h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "  # Second convolutional layer -- maps 32 feature maps to 64.\n",
    "  W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "  b_conv2 = bias_variable([64])\n",
    "  h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "  # Second pooling layer.\n",
    "  h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "  # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n",
    "  # is down to 7x7x64 feature maps -- maps this to 1024 features.\n",
    "  W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "  b_fc1 = bias_variable([1024])\n",
    "\n",
    "  h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "  h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "  # Dropout - controls the complexity of the model, prevents co-adaptation of\n",
    "  # features.\n",
    "  keep_prob = tf.placeholder(tf.float32)\n",
    "  h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "  # Map the 1024 features to 10 classes, one for each digit\n",
    "  W_fc2 = weight_variable([1024, 10])\n",
    "  b_fc2 = bias_variable([10])\n",
    "\n",
    "  y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "  return y_conv, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare placeholders for (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create the model\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# Define loss and optimizer\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the graph for the deep net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_conv, keep_prob = deepnn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_batchsize = 50\n",
    "mnist_steps = 5000\n",
    "epsilon = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define cross_entropy, optimizer, accuracy etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(epsilon).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "check_step = 1000\n",
    "\n",
    "# start time\n",
    "start = time.time()\n",
    "\n",
    "train_duration = 0.0\n",
    "train_duration_squared = 0.0\n",
    "check_duration = 0.0\n",
    "dtcheck = 0.0\n",
    "tcheck_prev = start\n",
    "\n",
    "# to store training history\n",
    "from collections import namedtuple\n",
    "\n",
    "ParamHeader = ['Timestamp', 'Info', 'Batch_size', 'Num_steps', 'TestAccuracy', 'TotalTime', 'TestTime', 'TrainTime', 'MeanPerBatch', 'StDev']\n",
    "ParamEntry = namedtuple('ParamEntry', ParamHeader)\n",
    "\n",
    "param_entries = []\n",
    "param_entries.append(ParamHeader) \n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(mnist_steps):          \n",
    "      batch = mnist.train.next_batch(mnist_batchsize)\n",
    "\n",
    "      if i % check_step == 0 :\n",
    "        tcheck = time.time()\n",
    "        train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0}) \n",
    "        dtcheck = tcheck - tcheck_prev\n",
    "        nbatches = check_step if i > 0 else 0\n",
    "        t1batch = dtcheck/float(nbatches) if nbatches > 0 else 0\n",
    "        print('step {0:6d}, training accuracy {1:5.3f} ({2:5d} batches trained in {3:6.4f} s, i.e. {4:9.07f} s/batch)'\n",
    "              .format(i, train_accuracy, nbatches, dtcheck, t1batch))\n",
    "        tcheck_prev = time.time()\n",
    "        check_duration += (time.time() - tcheck)\n",
    "        \n",
    "      start_train = time.time()     # measure training time per batch\n",
    "      train_step_ = sess.run(train_step, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "      duration = time.time() - start_train   # measure training time per batch\n",
    "      train_duration += duration\n",
    "      train_duration_squared += duration * duration\n",
    "    \n",
    "    start_test = time.time()\n",
    "    param_accuracy = accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})\n",
    "    test_duration = time.time() - start_test\n",
    "    total_runtime = time.time()\n",
    "\n",
    "    mn = train_duration / mnist_steps\n",
    "    vr = train_duration_squared / mnist_steps - mn * mn\n",
    "    sd = math.sqrt(vr)\n",
    "    print('\\ntest accuracy %g' % param_accuracy)\n",
    "    print('run in %g s, test: %g s, checks: %g s, train: %g s' % \n",
    "         (total_runtime, test_duration, check_duration, train_duration))\n",
    "    print('mean per batch %g +/- %g s' % (mn, sd))\n",
    "    param_entries.append(ParamEntry(\n",
    "                         datetime.now(), \"mnist\", mnist_batchsize, mnist_steps, param_accuracy, \n",
    "                         total_runtime, test_duration, train_duration, mn, sd))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store training results in .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../app')\n",
    "\n",
    "import data.storeincsv as incsv\n",
    "\n",
    "CSV_FILE = \"mnist_training.csv\"\n",
    "incsv.store_data_in_csv(CSV_FILE, param_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
